{
  "articles": [
    {
      "path": "data.html",
      "title": "Data",
      "description": "This section introduces my data and how I cleaned up it for later analysis.",
      "author": [],
      "contents": "\n\nContents\nData Loading and Organization\nExploratory Data Analysis\nData Cleaning\nTrait Simulation\n\nThroughout this site, I will go through examples of how to perform statistical techniques to better understand genetic data. To do this, I will rely on a publicly available dataset I downloaded from R-bloggers (Lima 2017), which includes 323 individuals (110 Chinese, 105 Indian and 108 Malay) and 2,527,458 SNPs.\nData Loading and Organization\nThe following code chunks outline the steps of importing the genetic data.\nLoad libraries:\n\n\nlibrary(snpStats)\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(NatParksPalettes)\nlibrary(parallel)\nlibrary(GGally)     \n\n\nLoad data for the Chinese, Indian, and Malay individuals and combine them into one SnpMatrix. This process uses read.plink(), which reads a genotype matrix, information on the study’s individuals, and information on the SNPs.\n\n\nload(\"GWAStutorial-master/conversionTable.RData\")\n\npathM <- paste(\"GWAStutorial-master/public/Genomics/108Malay_2527458snps\", c(\".bed\", \".bim\", \".fam\"), sep = \"\")\nSNP_M <- read.plink(pathM[1], pathM[2], pathM[3])\n\npathI <- paste(\"GWAStutorial-master/public/Genomics/105Indian_2527458snps\", c(\".bed\", \".bim\", \".fam\"), sep = \"\")\nSNP_I <- read.plink(pathI[1], pathI[2], pathI[3])\n\npathC <- paste(\"GWAStutorial-master/public/Genomics/110Chinese_2527458snps\", c(\".bed\", \".bim\", \".fam\"), sep = \"\")\nSNP_C <- read.plink(pathC[1], pathC[2], pathC[3])\n\nSNP <- rbind(SNP_M$genotypes, SNP_I$genotypes, SNP_C$genotypes)\n\n\nExecute additional data preparation steps recommended by R-bloggers site.\n\n\n# Take one bim map (all 3 maps are based on the same ordered set of SNPs)\nmap <- SNP_M$map\ncolnames(map) <- c(\"chromosome\", \"snp.name\", \"cM\", \"position\", \"allele.1\", \"allele.2\")\n\n# Rename SNPs present in the conversion table into rs IDs\nmappedSNPs <- intersect(map$SNP, names(conversionTable))\nnewIDs <- conversionTable[match(map$SNP[map$SNP %in% mappedSNPs], names(conversionTable))]\nmap$SNP[rownames(map) %in% mappedSNPs] <- newIDs\n\n\nExploratory Data Analysis\nFirst, get information about the genotype data. As stated earlier, we have 323 individuals and 2,527,458 SNPs.\n\n\nSNP\n\nA SnpMatrix with  323 rows and  2527458 columns\nRow names:  M11062205 ... M11082406 \nCol names:  rs4477212 ... kgp22743944 \n\nNext, look at the information we have on the individuals in the study. Theoretically, this gives information on family relationships with pedigree, father, and mother, but the father and mother variables contain only missing values. We also have information on the individual’s binary sex, with 1 representing male and 2 female. The affected column represents if the individual had the trait of interest or not, but we are not given that information in this data set so we will simulate a trait later in this analysis.\n\n\nindividuals <- rbind(SNP_M$fam, SNP_I$fam, SNP_C$fam)\nhead(individuals)\n\n           pedigree    member father mother sex affected\nM11062205 M11062205 M11062205     NA     NA   2       NA\nM11050406 M11050406 M11050406     NA     NA   1       NA\nM11050407 M11050407 M11050407     NA     NA   1       NA\nM11070602 M11070602 M11070602     NA     NA   2       NA\nM11060912 M11060912 M11060912     NA     NA   2       NA\nM11060920 M11060920 M11060920     NA     NA   2       NA\n\nFinally, we can look at the information we have on each SNP. This tells us a few things:\nchromosome is the number chromosome (typically 1-23) that the SNP is located on.1 is the largest chromosome (most SNPs) and chromosome size typically decreases from there.\n\nsnp.name is the name of the SNP\ncM stands for centiMorgans, which is a unit for genetic distance. It represents an estimate of how far SNPs are from one another along the genome.\nposition tells us the base pair position of the SNP, with position being being the first nucleotide in our DNA sequence.This number restarts from 1 at each chromosome.\n\nallele.1 is one of the alleles at this SNP, here the minor allele.\nallele.2 is the other allele at this SNP, here the major allele.\n\n\nhead(map)\n\n            chromosome    snp.name cM position allele.1 allele.2\nrs4477212            1   rs4477212 NA    82154     <NA>        A\nkgp15717912          1 kgp15717912 NA   534247     <NA>        C\nkgp7727307           1  kgp7727307 NA   569624     <NA>        C\nkgp15297216          1 kgp15297216 NA   723918     <NA>        G\nrs3094315            1   rs3094315 NA   752566        G        A\nrs3131972            1   rs3131972 NA   752721        A        G\n\nData Cleaning\nOne useful piece of information not contained in the data is the minor allele frequency (MAF). This represents the frequency of the minor allele in the dataset. We can add this to our SNP information using the snpstats package and add MAF to map, our data frame that gives us SNP information.\n\n\n#calculate MAF\nmaf <- col.summary(SNP)$MAF\n\n# add new MAF variable to map\nmap <- map %>%\n  mutate(MAF = maf)\nhead(map)\n\n            chromosome    snp.name cM position allele.1 allele.2\nrs4477212            1   rs4477212 NA    82154     <NA>        A\nkgp15717912          1 kgp15717912 NA   534247     <NA>        C\nkgp7727307           1  kgp7727307 NA   569624     <NA>        C\nkgp15297216          1 kgp15297216 NA   723918     <NA>        G\nrs3094315            1   rs3094315 NA   752566        G        A\nrs3131972            1   rs3131972 NA   752721        A        G\n                    MAF\nrs4477212   0.000000000\nkgp15717912 0.000000000\nkgp7727307  0.001572327\nkgp15297216 0.000000000\nrs3094315   0.139318885\nrs3131972   0.233746130\n\nJust looking at the MAF for the first six SNPs in our data, we see that in some cases the minor allele frequency is 0. This means that the SNP is monomorphic - everyone in the dataset has the same genotype at these positions. We will remove these monomorphic SNPs - if everyone has the same alleles at a SNP, there is no variation and we cannot find an association between the minor allele and the trait.\nIt can also help to think about why we remove SNPs with a MAF of 0 in a mathematical way. If we are trying to fit a line between the trait of interest and SNP 1, we could model this in the following formats, with linear regression listed first and matrix notation second.\n\\[E[Y|\\text{SNP1}] = \\beta_0 + \\beta1 \\text{SNP1}\\]\n\\[E[\\bf{y}|\\bf{X}] = \\boldsymbol{\\beta} X\\]\nFurther exploring the matrix format, it would look like this:\n\\[X\\boldsymbol{\\beta} = \\begin{bmatrix}\n1 & 0 \\\\\n1 & 0 \\\\\n. & . \\\\\n. & . \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0\\\\\n\\beta_1 \\\\\n\\end{bmatrix}\\]\nExecuting this multiplication, we just get \\(1 * \\beta_0 = 0\\). The is problematic because we have linear dependence. You can get the column of minor allele counts by multiplying the intercept column by 0 - in other words, the minor allele count column is a linear combination of the intercept column. This makes our design matrix not be full rank, making \\(X^TX\\) not invertible and the least squares estimator not defined.\nGiven all these reasons, we remove SNPs with a MAF of 0 using the code below.\n\n\nmap <- map %>%\n  filter(maf >0 )\n\ndim(map)\n\n[1] 1651345       7\n\nAfter filtering, we have 1,651,345 SNPs remaining. Therefore, we removed 876,113 SNPs.\nHowever, we are not done cleaning the data. Below, when looking at the first six rows of map, we see two NA values for allele.1. In these rows, the minor allele frequency is not quite 0, but it is very small. In fact, in the first row of this data frame the MAF is 1/646, or 0.00157. This represents that out of the 646 alleles studied (2 alleles for each of the 323 people in the data), there was only one minor allele. On SNP 4 in the data, there were only 3 minor alleles (3/646 = 0.00465).\n\n\nhead(map)\n\n            chromosome    snp.name cM position allele.1 allele.2\nkgp7727307           1  kgp7727307 NA   569624     <NA>        C\nrs3094315            1   rs3094315 NA   752566        G        A\nrs3131972            1   rs3131972 NA   752721        A        G\nkgp6703048           1  kgp6703048 NA   754063        T        G\nkgp15557302          1 kgp15557302 NA   757691     <NA>        T\nkgp12112772          1 kgp12112772 NA   759036        A        G\n                    MAF\nkgp7727307  0.001572327\nrs3094315   0.139318885\nrs3131972   0.233746130\nkgp6703048  0.027950311\nkgp15557302 0.004658385\nkgp12112772 0.026397516\n\nWhy did the study put NA values instead of the one minor allele found? Perhaps they are worried of a machine reading error given that the minor allele was detected only a couple of times, or maybe there was another reason. To better understand these missing values, I created the density plots below.\n\n\nmap %>%\n  mutate(missing = as.factor(case_when(is.na(allele.1) ~ \"Allele 1 missing\", \n                             TRUE ~ \"Allele 1 Recorded\"))) %>%\n  ggplot(aes(x=MAF))+\n  geom_density(alpha = 0.5, fill = \"cadetblue3\")+\n  theme_classic()+\n  facet_wrap(~missing, scales = \"free\")+\n  labs(x = \"Minor allele frequency\", y = \"density\")+\n  theme(axis.title = element_text(family = \"mono\"), \n        strip.background = element_blank(), \n        strip.text = element_text(family = \"mono\"))\n\n\n\nThese plots really surprised me. Initially my plan was just to remove all SNPs with a MAF < 1%, figuring that would filter out all SNPs with an NA value for allele 1. However, in the plot above on the left we see that while the majority of allele 1 missing SNPs have a MAF < 1%, some have a MAF close to 11%, meaning about 71/646 minor alleles were detected yet an NA value was still recorded. While I have a very minimal understanding of gene reading machinery, I would not guess that this NA is not a machine reading error but rather that something else is going on. Given this information, I decided to compromise and remove all SNPs with a MAF < 3%, as well as all other SNPs with an NA value for allele 1. This brings us from 1,651,345 SNPs to 1,293,100 SNPs.\n\n\nmap.clean <- map %>%\n  filter(MAF > 0.03, \n         !is.na(allele.1))\ndim(map.clean)\n\n[1] 1293100       7\n\nOne downside to this whole process of removing SNPs with small MAFs is that a major goal of GWAS studies is detect to rare variations on SNPs that could be associated with the trait of interest. Removing SNPs where the MAF is small may result in removing critical data to the study. This is trade off emphasizes that having more people in your GWAS is helpful and important in forming meaningful results about potentially rare variants.\nBefore moving on, we must complete one final data cleaning step. The snpstats package uses a format in which genotypes are coded as 01, 02, and 03, with 00 representing missing values.\n\n\nSNP@.Data[1:5,1:5]\n\n          rs4477212 kgp15717912 kgp7727307 kgp15297216 rs3094315\nM11062205        03          03         03          03        03\nM11050406        03          03         03          03        03\nM11050407        03          03         03          03        03\nM11070602        03          03         03          03        02\nM11060912        03          03         03          03        03\n\nWe will convert this to a 0, 1, and 2 format. Now the matrix represents the number of major alleles each person has at each SNP.\n\n\nX <- as(SNP, \"numeric\")\nX[1:5, 1:5]\n\n          rs4477212 kgp15717912 kgp7727307 kgp15297216 rs3094315\nM11062205         2           2          2           2         2\nM11050406         2           2          2           2         2\nM11050407         2           2          2           2         2\nM11070602         2           2          2           2         1\nM11060912         2           2          2           2         2\n\nWe also must remove the SNPs with a MAF < 3% and those missing allele 1 from our genotypic matrix X.\n\n\nX.clean <- X[,colnames(X) %in% map.clean$snp.name]\ndim(X.clean)\n\n[1]     323 1293100\n\nTrait Simulation\nAs discussed earlier, the affected column in our individuals dataset is completely missing values. Therefore, for the purposes of demonstrating how to conduct a GWAS, we will simulate a trait using a random SNP of interest. I randomly chose SNP rs3131972. This SNP is located on chromosome 1 near gene FAM87B. A is the major allele in comparison to G, matching what we see in our data. The minor allele frequency is 23.4%.\n\n\nmap %>%\n  filter(snp.name == \"rs3131972\")\n\n          chromosome  snp.name cM position allele.1 allele.2\nrs3131972          1 rs3131972 NA   752721        A        G\n                MAF\nrs3131972 0.2337461\n\nTo create a quantitative trait y we will use information dependent on the genotype at this SNP plus random noise that is normally distributed with a mean of 0 and standard deviation of 1.\n\n\nn <- nrow(X)\nset.seed(494)\ny <- X[,'rs3131972'] + rnorm(n, 0, 1)\nhead(y)\n\nM11062205 M11050406 M11050407 M11070602 M11060912 M11060920 \n0.2082684 0.6267809 1.3422648 0.1696396 2.9450827 2.8350451 \n\nCheck out the GWAS tab to see how we will use this trait!\n\n\n\nLima, Francisco. 2017. “Genome-Wide Association Studies in r: R-Bloggers.” R. https://www.r-bloggers.com/2017/10/genome-wide-association-studies-in-r/.\n\n\n\n\n",
      "last_modified": "2022-12-10T15:04:57-06:00"
    },
    {
      "path": "gwas.html",
      "title": "Genome Wide Association Studies (GWAS)",
      "description": "What a GWAS is and how to do it in RStudio and PLINK.",
      "author": [],
      "contents": "\n\nContents\nGWAS in RStudio\nAnalyze chromosome 1\nAnalyze all chromosomes\n\nGWAS in PLINK\n\n\n\n\n\n\n\nOn the home page, I introduced genetic variants. We can study these genetic variants with something known as a Genome-Wide Association Study (GWAS). The overarching goal of a GWAS is to help us understand which SNPs might be causally associated with our trait of interest, which can be particularly important and helpful when trying to find a cure for a disease or understand why certain people get that disease.\n\nUsing the trait we developed in the Data tab, we can implement marginal regression to run GWAS on our data. If you haven’t yet looked through the Data section, it would be useful to do so now in order to understand the context of this GWAS.\nGWAS in RStudio\nWe developed our trait of interest around causal SNP, rs3131972, which we know to be located on chromosome 1. However, in a real genetic study we would not know where the casual SNP(s) we are looking for are located. Therefore, we’d need to run a GWAS to see if there are variants are associated with the trait of interest and if so, where. To do this we will use marginal regression.\nFor each SNP, we will fit a model with the SNP as the single independent variable and the trait of interest as the dependent variable. Looking at our first three SNPs, the models can be created like this:\n\n\nsnp1mod <- lm(y ~ X.clean[,1])\nsnp2mod <- lm(y ~ X.clean[,2])\nsnp3mod <- lm(y ~ X.clean[,3])\n\ntidy(snp1mod)\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic      p.value\n  <chr>           <dbl>     <dbl>     <dbl>        <dbl>\n1 (Intercept)   -0.0869     0.308    -0.282 0.778       \n2 X.clean[, 1]   1.00       0.172     5.82  0.0000000144\n\nEach of these models produces an estimate for the coefficient on the SNP. For example, the coefficient for snp1mod is 1.00. The way we might interpret this is that for every additional minor allele (G for example) that you carry at that position, the trait of interest changes by about 1.00 units. If the trait we were measuring was height, we would expect your height to increase about 1.00 inches for every additional minor allele (a value of either 0, 1, or 2) at SNP 1. This is obviously pretty extreme, but you understand the idea.\nAnalyze chromosome 1\nObviously, we cannot do the process above by hand for over one million SNPs. However, we can do this with a loop! We will start first with all SNPs located on chromosome 1.\nFirst, pick out these SNPs using which().\n\n\nchromosome1.snps <- which(map.clean$chromosome == 1)\nlength(chromosome1.snps)\n\n[1] 100766\n\nNext, loop through each of the SNPs, fitting a linear regression model at each one. For each model, we’ll record the estimates (betas), standard errors (ses), test statistics (tstats) and p-values (pvals) for the coefficient of interest, which is the slope.\n\n\n# set up empty vectors for storing the results\nbetas <- c()\nses <- c()\ntstats <- c()\npvals <- c()\n\n# loop through SNPs in chromosme 1\nfor(i in chromosome1.snps){\n  # fit model\n  mod <- lm(y ~ X.clean[,i])\n  # get coefficient information\n  coefinfo <- tidy(mod)\n  # record estimate, SE, test stat, and p-value\n  betas[i] <- coefinfo$estimate[2]\n  ses[i] <- coefinfo$std.error[2]\n  tstats[i] <- coefinfo$statistic[2]\n  pvals[i] <- coefinfo$p.value[2]\n}\n\n\nAfter completing the loop, we add our results to our map data frame that contains information about each SNP:\n\n\n# start with the map info for the chr 1 SNPs\nchr1.results <- map.clean %>%\n  filter(chromosome == 1)\n\n# then add betas, SEs, etc.\nchr1.results <- chr1.results %>%\n  mutate(Estimate = betas,\n         Std.Error = ses,\n         Test.Statistic = tstats,\n         P.Value = pvals)\n\n# look at results\nhead(chr1.results)\n\n            chromosome    snp.name cM position allele.1 allele.2\nrs3094315            1   rs3094315 NA   752566        G        A\nrs3131972            1   rs3131972 NA   752721        A        G\nexm2268640           1  exm2268640 NA   762320        T        C\nkgp3709449           1  kgp3709449 NA   771967        A        G\nkgp15275285          1 kgp15275285 NA   774047        A        G\nkgp5225889           1  kgp5225889 NA   779322        G        A\n                   MAF   Estimate Std.Error Test.Statistic\nrs3094315   0.13931889  0.9782628 0.1686493       5.800573\nrs3131972   0.23374613  0.9983550 0.1448057       6.894448\nexm2268640  0.09627329  0.5434766 0.2178257       2.495007\nkgp3709449  0.11490683  0.8697466 0.1810295       4.804448\nkgp15275285 0.14241486 -0.2383669 0.1599812      -1.489968\nkgp5225889  0.12229102  0.8810804 0.1769315       4.979782\n                 P.Value\nrs3094315   1.583433e-08\nrs3131972   2.877123e-11\nexm2268640  1.309910e-02\nkgp3709449  2.389118e-06\nkgp15275285 1.372146e-01\nkgp5225889  1.041390e-06\n\nchr1.results %>%\n  filter(snp.name == 'rs3131972')\n\n          chromosome  snp.name cM position allele.1 allele.2\nrs3131972          1 rs3131972 NA   752721        A        G\n                MAF Estimate Std.Error Test.Statistic      P.Value\nrs3131972 0.2337461 0.998355 0.1448057       6.894448 2.877123e-11\n\nLastly, we can plot the results. We take the log of the p-value in order to better identify SNPs with small p-values, and then take the negative of this to flip the plot and make it look like the typical Manhattan plot. We see a gap in the middle of the plot where the centromere of chromosome 1 is located. Centromeres are difficult to genotype so we don’t get any data in this area. The causal SNP is easy to spot colored in navy blue with a \\(-\\text{log}_{10}\\)(p-value) close to 12.\n\n\nchr1.results %>%\n  mutate(minuslogp = -log10(P.Value), \n         causalSNP = as.factor(case_when(snp.name == \"rs3131972\" ~ 1, \n                               TRUE ~ 0))) %>%\n  ggplot(aes(x = position, y = minuslogp, color = causalSNP)) +\n  geom_point() + \n  scale_color_manual(values = c(\"goldenrod\", \"navy\"))+\n  labs(x = 'position (bp)', y = expression(paste('-log'[10],'(p-value)'))) + \n  scale_x_continuous(labels = scales::comma)+\n  theme_classic()+\n  theme(legend.position = \"none\")\n\n\n\nAnalyze all chromosomes\nFinally, we can analyze all chromosomes. To do this, we simply loop over the SNPs in all chromosomes instead of just those in chromosome 1. The problem with this code is that on my personal computer it takes over an hour to run and even on faster Macalester computers it takes close to 30 minutes. As a result, I will not actually run it here and instead talk about a solution to this problem later on.\n\n\n# set up empty vectors for storing results\nbetas <- c()\nses <- c()\ntstats <- c()\npvals <- c()\n\n# loop through all SNPs\nfor(i in 1:ncol(X.clean)){ \n  # fit model\n  mod <- lm(y ~ X.clean[,i])\n  # get coefficient information\n  coefinfo <- tidy(mod)\n  # record estimate, SE, test stat, and p-value\n  betas[i] <- coefinfo$estimate[2]\n  ses[i] <- coefinfo$std.error[2]\n  tstats[i] <- coefinfo$statistic[2]\n  pvals[i] <- coefinfo$p.value[2]\n}\n\n\n\n\n# start with the map info\nall.results <- map.clean\n\n# then add betas, SEs, etc.\nall.results <- all.results %>%\n  mutate(Estimate = betas,\n         Std.Error = ses,\n         Test.Statistic = tstats,\n         P.Value = pvals)\n\n\nHad we ran the code above, we could plot the results in a similar matter as we did for chromosome one, just making one small change to the code. Instead of plotting position along the x axis, we group with an interaction between position and chromosome. This is due to position restarting over again at each chromosome, so it prevents all the points from being plotted on top of one another. The final manhattan plot would then look something like the image below.\n\n\nall.results %>%\n  mutate(minuslogp = -log10(P.Value),\n         chr = as.factor(chromosome)) %>%\n  ggplot(aes(x = chr, y = minuslogp, group = interaction(chr, position), color = chr)) + #interaction prevents chromosomes from overlapping\n  geom_point(position = position_dodge(0.8)) +\n  scale_color_manual(values=natparks.pals(\"DeathValley\",22))+\n  labs(x = 'chromosome', y = expression(paste('-log'[10],'(p-value)')))+\n  theme_classic()+\n  theme(legend.position = \"none\")\n\n\n\nGWAS in PLINK\nWhile the code above is fairly simple, it is not efficient. This is where the software PLINK can help. PLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner. To learn more about PLINK and how to download and use it, check out this site (“Whole Genome Association Analysis Toolset,” n.d.), which I relied heavily on. Once downloaded onto your computer, PLINK runs from the terminal. We can run a GWAS in PLINK in a matter of a couple seconds instead of 30-60 minutes.\nFor the purposes of demonstrating PLINK, I simulated a trait not associated with any particular SNP, but that is associated with population structure. This will show us how a Manhattan plot with no associated SNPs compares to the one above, where we know a causal SNP exists. Creating a trait associated with the population structure will also be useful for analysis later in the PCA section.\nTo get the data ready to run in PLINK, I started by merging all 9 files (the .bim, .bam, and .fam for each of the 3 sub-populations). I listed the name of each of these files in .txt file with the structure shown in the image below.\n\nIn the terminal and in the folder where my data was stored, I then ran the command ./plink --merge-list allfiles.txt --make-bed --out rbloggersComb. This combines the data into three files that PLINK can use (rbloggersComb.bed, rbloggersComb.bim, rbloggersComb.fam).\nMy next step was to create a trait in RStudio. To do this, I loaded in the data in the same way that I did in the Data section, only this time adding a couple additional lines to extract information about each individual and SNP information.\n\n\nrbloggers_fam <- rbind(SNP_M$fam, SNP_I$fam, SNP_C$fam)\nrbloggers_map <- rbind(SNP_M$map, SNP_I$map, SNP_C$map)\n\n\nI then selected the pedigree and member columns from the rbloggers_fam table and column-binded on a trait that varies by population. I wrote this new dataset (which has one row for each of the 323 study participants) to the folder where the data is stored.\n\n\nset.seed(494)\nrbloggers_poptrait <- cbind(rbloggers_fam %>% select(1:2), trait = c(rnorm(n = 108, mean = 0, sd = 1), rnorm(105, -1, sd = 1), rnorm(110, 1, 1)))\n\nhead(rbloggers_poptrait)\n\n           pedigree    member      trait\nM11062205 M11062205 M11062205 -1.7917316\nM11050406 M11050406 M11050406 -0.3732191\nM11050407 M11050407 M11050407 -0.6577352\nM11070602 M11070602 M11070602 -0.8303604\nM11060912 M11060912 M11060912  1.9450827\nM11060920 M11060920 M11060920  0.8350451\n\n\n\nwrite_delim(rbloggers_poptrait, \"rbloggersSep/rbloggers_poptrait\")\n\n\nIn PLINK, I then ran the command ./plink --bfile rbloggersComb --assoc --adjust --pheno rbloggers_poptrait --out as2. The command and subsequent output are shown below.\n\nThis command creates two files, one in which GWAS in ran without any adjustments (as2.qassoc) and one where it is ran with adjustments (as2.qassoc.adjusted). The file without adjustments returns a row for each of our original 2,527,458 SNPs. Monomorphic SNPs return a NA value for the information we care about (the t-statistic and p-value of the test). The file with adjustments removes all monomorphic SNPs from the data for us. It returns an unadjusted p-value for each SNP (UNADJ) and an adjusted p-value (GC). These p-values are quite different, as shown by the density plots below.\n\n\nrbloggersSep_adjusted <- read_table(\"/Users/erinfranke/Desktop/MACStats/Statistical Genetics/StatGenWillErin/rbloggersSep/as2.qassoc.adjusted\")\nhead(rbloggersSep_adjusted)\n\n# A tibble: 6 × 10\n    CHR SNP            UNADJ      GC     BONF     HOLM SIDAK…¹ SIDAK…²\n  <dbl> <chr>          <dbl>   <dbl>    <dbl>    <dbl> <chr>   <chr>  \n1     2 kgp14454942 7.52e-31 9.84e-7 1.24e-24 1.24e-24 INF     INF    \n2     2 kgp14377296 9.16e-28 4.53e-6 1.51e-21 1.51e-21 INF     INF    \n3     2 kgp5564602  6.80e-27 6.91e-6 1.12e-20 1.12e-20 INF     INF    \n4     2 kgp14779806 6.95e-27 6.94e-6 1.15e-20 1.15e-20 INF     INF    \n5    13 rs10507688  1.30e-26 7.91e-6 2.14e-20 2.14e-20 INF     INF    \n6     2 rs4149433   2.32e-26 8.92e-6 3.82e-20 3.82e-20 INF     INF    \n# … with 2 more variables: FDR_BH <dbl>, FDR_BY <dbl>, and\n#   abbreviated variable names ¹​SIDAK_SS, ²​SIDAK_SD\n\nggplot(rbloggersSep_adjusted, aes(x=UNADJ))+\n  geom_density(fill = \"cadetblue4\")+\n  theme_classic()+\n  labs(title = \"Distribution of p-values WITHOUT adjustment\", x = \"Unadjusted p-value\", y = \"Density\")+\n  theme(plot.title.position = \"plot\", \n        plot.title = element_text(family = \"mono\"), \n        axis.title = element_text(family = \"mono\"))\n\n\nggplot(rbloggersSep_adjusted, aes(x=GC))+\n  geom_density(fill = \"cadetblue4\")+\n  theme_classic()+\n  labs(title = \"Distribution of p-values WITH adjustment\", x = \"Adjusted p-value\", y = \"Density\")+\n  theme(plot.title.position = \"plot\", \n        plot.title = element_text(family = \"mono\"), \n        axis.title = element_text(family = \"mono\"))\n\n\n\nWe see that in general SNPs tend to have much more significant p-values in the density plots WITHOUT adjustments than in the density plot WITH adjustments. Why is this the case? As discussed in the Introduction of The Power of Genomic Control, the adjusted p-values listed under the GC column account for nonindependence in a case-control sample caused by population stratification and cryptic relatedness (Bacanu, Devlin, and Roeder 2008). Population stratification means systematic ancestry differences between cases and controls. While we do not know the cases and controls in this analysis given the provided trait was all missing values, it is highly likely population stratification exists. Cryptic relatedness means sample structure in our data due to distant relatedness among samples with no known family relationships. We may also have family structure in our data, meaning sample structure due to familial relatedness among samples (Price et al. 2010). I believe there are ways to identify and remove related samples in the data, but I have not yet learned how to do this. Nonetheless, it is clear some combination of population structure, cryptic relatedness, and family structure has significantly inflated the p-values received from each test for significance between a SNP and our trait of interest. As a result, we will proceed with the adjusted p-values in the GC column.\nWith these p-values, we can create a Manhattan plot in a similar manner that we did earlier, just first adding SNP position data to our GWAS results.\n\n\nrbloggersSep_adjusted <- rbloggersSep_adjusted%>%\n  mutate(CHR = as.integer(CHR)) %>%\n  left_join(rbloggers_map %>%\n              dplyr::select(snp.name, position, chromosome), by = c(\"SNP\" = \"snp.name\", \"CHR\" = \"chromosome\"))\n\n\n\n\nrbloggersSep_adjusted %>%\n  mutate(minuslogp = -log10(GC),\n         CHR = as.factor(CHR)) %>%\n  ggplot(aes(x = CHR, y = minuslogp, group = interaction(CHR, position), color = CHR)) + \n  geom_point(position = position_dodge(0.8)) + \n  labs(x = 'chromosome', y = expression(paste('-log'[10],'(p-value)')))+\n  theme_classic()+\n  scale_color_manual(values=natparks.pals(\"DeathValley\",24))+\n  theme(legend.position = \"none\")\n\n\n\nOverall, this manhattan plot looks fairly similar to the original one we created earlier. However, the most significant p-value in this plot has a \\(-\\text{log}_{10}(\\text{p-value})\\) of only about 6. This is expected given we did not simulate the trait based on any one particular SNP and we know that none of the SNPs are causally located with our trait of interest. As previously mentioned, in a real genetic study we do not know whether or not any of our SNPs will be causally associated with our trait of interest. Therefore, we need some kind of threshold to determine what SNPs we should look more closely at (if any). To learn more about how to do this, click on the Multiple Testing tab!\n\n\n\nBacanu, Silviu-Alin, B. Devlin, and Kathryn Roeder. 2008. “The Power of Genomic Control.” The American Journal of Human Genetics. Cell Press. https://www.sciencedirect.com/science/article/pii/S0002929707635459.\n\n\nPrice, Alkes L., Noah A. Zaitlen, David Reich, and Nick Patterson. 2010. “New Approaches to Population Stratification in Genome-Wide Association Studies.” Nature Reviews Genetics 11 (7): 459–63. https://doi.org/10.1038/nrg2813.\n\n\n“Whole Genome Association Analysis Toolset.” n.d. PLINK: Whole Genome Data Analysis Toolset. https://zzz.bwh.harvard.edu/plink/.\n\n\n\n\n",
      "last_modified": "2022-12-10T15:42:38-06:00"
    },
    {
      "path": "index.html",
      "title": "Welcome",
      "description": "This website represents a summary of what I have learned in Stat 494 Statistical Genetics!\n",
      "author": [],
      "contents": "\nIn this website, I talk about how to execute genetic wide association studies in RStudio and the software PLINK, multiple testing, genetic ancestry and principal component analysis, and some of the ethics behind statistical genetics. I recommend following along in the order of tabs listed above.\nTo get started, you’ll have to have a little background on genetics. Check out the following “need-to-know” information about genetic variants (what the study of statistical genetics is based around).\n\nTo learn more, switch over to the Data tab!\nAcknowledgements\nI would like to give a big thank you to my professor Kelsey Grinde for making this website possible and getting me interested in the field of statistical genetics :)\n\n\n\n",
      "last_modified": "2022-12-10T14:50:15-06:00"
    },
    {
      "path": "multipletesting.html",
      "title": "Multiple Testing",
      "description": "What it is, why we need it, and how to do it in RStudio and PLINK.",
      "author": [],
      "contents": "\n\nContents\nIntroducing Multiple Testing\nHypothesis Testing Background\nBack to the threshold\n\nAccounting for correlation\nNearby SNPs are correlated\nGetting a threshold with simulation\n\nUsing PLINK for Multiple Hypothesis Testing\n\n\n\n\n\n\n\nIntroducing Multiple Testing\nIn the plot Manhattan plot created using RStudio we were able to visually pick out the SNP that we know to be associated with the trait of interest. However, in a real GWAS we do know which SNPs are associated with the trait of interest so this is not the case. As a result, we need a way to decide if a p-value is small enough to be statistically significant and/or to warrant follow-up analysis.\nReading through some scientific articles, I have seen a wide variety of thresholds be used. These thresholds largely fall between \\(5 \\times 10^{-7}\\) on the higher end and \\(1 \\times 10^{-9}\\) on the lower end, which are obviously much smaller than the conventionally accepted \\(0.05\\) threshold. Why is this, and how can we determine which threshold to use?\nHypothesis Testing Background\nBefore getting into thresholds, it is important to understand the basics of hypothesis testing. The goal is to make a decision between two conflicting theories, which are labeled as the null and alternative hypothesis. In this situation, the null hypothesis \\(H_0\\) is that the specific SNP is not associated with the trait of interest. The alternative hypothesis \\(H_A\\) is that the specific SNP is associated with the trait of interest. Each SNP is tested independently for a relationship with the trait, and if the p-value resulting from the test falls below the chosen threshold then \\(H_0\\) can be rejected.\nThe example below shows a test of SNP number 830,000 in this dataset. Its p-value is \\(0.41\\), indicating that at the threshold of \\(5 \\times 10^{-7}\\) we would fail to reject the null hypothesis and conclude this SNP to not be associated with our trait of interest (which we rightfully know to be true in this simulation).\n\n\nset.seed(453)\nsnp1mod <- lm(y ~ X.clean[,830000])\ntidy(snp1mod)\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)         1.72       0.154    11.1   1.45e-24\n2 X.clean[, 830000]  -0.0833     0.101    -0.821 4.12e- 1\n\nAnother key piece of information related to hypothesis testing is errors. Ideally, we want to reject \\(H_0\\) when it is wrong and accept \\(H_0\\) when it is right, but unfortunately this does not always happen. We call rejecting the null hypothesis \\(H_0\\) when it is true a type I error, and failing to reject the alternative hypothesis when it is true a type II error. For a given test, the probability of making a type I error is \\(\\alpha\\), our chosen threshold, meaning the probability of making a type I error is in our control. Type II errors are dependent on a larger number of factors. The probability of committing a type II error is 1 minus the power of the test, where power is the probability of correctly rejecting the null hypothesis. To increase the power of a test, you can increase \\(\\alpha\\) (though this increases type I error rate) and/or increase the sample size (here the number of people in our study).\nIn the context of the data, would be it better to commit a type I or type II error? In this situation, committing a type I error would mean concluding a SNP is associated with the trait of interest when in reality it is not, and committing a type II error would mean concluding a SNP is has no relationship to the trait of interest when it actually does. A harm of the type I error is investing additional time and money into studying that particular SNP and/or incorrectly giving those impacted by the disease hope that you are discovering potential causes of it. For a type II error, you are denoting SNPs associated with the trait of interest as insignificant and passing up key information that could be useful in solving a disease. The harms of both are highly unfortunate, however I would lean more on the side of minimizing the occurrence of type II errors which in turn would mean using a threshold on the slightly higher end.\nBack to the threshold\nAs mentioned earlier, thresholds in genetic studies commonly fall between \\(5 \\times 10^{-7}\\) and \\(1 \\times 10^{-9}\\). Why is this?\nLet’s say we are running tests for association with our trait of interest on just the 100,766 SNPs in chromosome 1 and for simplicity that the tests are independent. If we are conducting a test on just the first of those SNPs and use \\(\\alpha = 0.05\\), the probability of making a type I error is 0.05. However, the probability of making a type I error in all 100,766 tests needed for the SNPs of the first chromosome is\n\\[P(\\text{at least 1 Type I error}) = 1 - P(\\text{no error test 1}) \\times... \\times P(\\text{no error test 100,766})\\]\n\\[ = 1 - [1-0.05]^{100,766} = \\text{essentially } 1\\]\nWith a threshold of 0.05 and independent tests, the probability of having at least one type I error (or the family-wise error rate (FWER)) for SNPs on chromosome 1 is essentially 100% as shown above. This makes it obvious that a smaller threshold is needed. If we were to use \\(5 \\times 10^{-7}\\), this probability would fall to right around 0.05!\n\\[ = 1 - [1-0.0000005]^{100,766} = 0.04913\\]\nThe threshold \\(5 \\times 10^{-7}\\) didn’t just appear out of thin air. Statisticians came up with this threshold using what is called the Bonferroni Correction, which is a technique that gives a new significance threshold by dividing the desired family wise error rate by the number of tests conducted. Therefore, with our data if we wanted a 5% probability of a type I error across all chromosomes we would decrease the threshold to \\(3.867 \\times 10^{-8}\\) as there are 1,651,345 polymorphic SNPs in this dataset.\n\\[ \\text{Bonferroni threshold} = \\frac{\\text{FWER}}{\\text{ # tests}} = \\frac{0.05}{1,651,345} = 3.03 \\times 10^{-8}\\]\nAccounting for correlation\nNearby SNPs are correlated\nWe just came to the conclusion that for our dataset we would use a threshold of \\(3.03 \\times 10^{-8}\\) to determine whether or a not SNP may have a relationship with the trait of interest. However, in doing this we made one key assumption, which is that our tests are independent from one another. Unfortunately, this is certainly not the case due to linkage disequalibrium, which as stated in Science Direct is the idea that two markers in close physical proximity are correlated in a population and are in association more than would be expected with random assortment (“Linkage Disequilibrium,” n.d.). Essentially, SNPs next to each other are much more similar than SNPs far away from each other. This concept of correlated SNPs is demonstrated by the plot below, which plots the linkage disequilibrium matrix for the first 200 SNPs on chromosome 1.\n\n\nchr1_200 <- SNP[1:323, 1:200]\nhapmap.ld <- ld(chr1_200, depth = 199, stats = \"R.squared\", symmetric = TRUE)\ncolor.pal <- natparks.pals(\"Acadia\", 10)\nimage(hapmap.ld, lwd = 0, cuts = 9, col.regions = color.pal, colorkey = TRUE)\n\n\n\nIf you look closely, you can see that along the diagonal there is a higher concentration of orange, meaning that neighboring SNPs are highly correlated with one another. However, this is a little hard to see because of all the white lines. The white lines are occurring at monomorphic SNPs. If we wanted to calculate the correlation between two SNPs (X and Y) we would use the following equation:\n\\[r = \\frac{\\sum_{i=1}^n(x_i -\n\\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2\n\\sum_{i=1}^n(y_i - \\bar{y})^2}}\\]\nIf we consider SNP X to be monomorphic, that means there are 0 copies of the minor allele for all people in our dataset. In the equation above, that means \\(x_1 = ... = x_{323} = 0\\). This means the sample average \\(\\bar{x}\\) is also 0 and thus \\(x_i - \\bar{x} = 0 - 0 = 0\\) for individuals \\(i = 1, \\dots, 323\\). Plugging this information in, we get an undefined correlation, which is what all the white lines represent.\n\\[r = \\frac{\\sum_{i=1}^{323} 0 \\times (y_i -\n\\bar{y})}{\\sqrt{0 \\times \\sum_{i=1}^{323}(y_i - \\bar{y})^2}} =\n\\frac{0}{0}\\]\nRemoving the monomorphic SNPs from our LD matrix gets rid of over 120 monomorphic SNPs and better shows how highly correlated nearby SNPs are.\n\n\n#get monomorphic SNPs only\nmaf_chr1_200 <- col.summary(chr1_200)$MAF\nmono <- which(maf_chr1_200 == 0)\n\n# calculate LD on polymorphic SNPs only\nhapmap.ld.nomono <- ld(chr1_200[,-mono], depth = 199-length(mono), stats = \"R.squared\", symmetric = TRUE)\n\n# plot \nimage(hapmap.ld.nomono, lwd = 0, cuts = 9, col.regions = color.pal, colorkey = TRUE)\n\n\n\nGetting a threshold with simulation\nTo see how the correlation impacts our threshold for a given FWER, we can use simulation. The process for simulation goes as follows:\nSimulate a null trait, meaning a trait not associated with any of the SNPs.\nRun GWAS to test the association between the simulated null trait and each SNP in our dataset. After that record the smallest p-value from this GWAS.\nRepeat steps 1 and 2 many times, typically 1,000-10,000 times in professional genetic studies.\nLook at the p-values saved from those simulation replicates. Sort them from smallest to largest and find the number at which 5% (desired FWER) of p-values are smaller than that number. This is the significance threshold.\nThis process is very computationally expensive, especially when 10,000 replications are completed. Each dataset has a different level of correlation between SNPs which is why there is no one widely accepted threshold for a given number of SNPs. Running 1,000 replications on Macalester’s computer takes about 29 minutes per replication, or just over 20 days total. Thus, 10,000 replications would take close to 7 months, which is clearly not computationally efficient. As a result, researchers will send code to remote computers or complete their multiple testing process in a more efficient software such as PLINK. The code below shows how we would run the the simulation in RStudio 1000 times for the entire dataset, but I will not actually run it. If I did run this code, one way I could minimize computational time slightly would be to use the mclapply() function from the parallel package. This allows computation to be split across the two cores of my computer. If your computer has more cores, this could potentially help speed up computation by a factor of the number of cores your computer has (e.g. 8x faster if you have 8 cores).\n\n# make genotype matrix into form of 0, 1, and 2s\nsnp <- as(SNP, \"numeric\")\ndim(snp)\n\n# calculate MAF\nmaf <- col.summary(SNP)$MAF\n\n# find monomorphic SNPs\nmonomorphic <- which(maf == 0) \n\n# filter genotype matrix to remove monomorphic SNPs\nsnp <- snp[,-monomorphic]\n\n# check the dimensions after filtering\ndim(snp)\n\ndo_one_sim<- function(i){\n  \n  # simulate null trait\n  y <- rnorm(n = 323, mean = 0, sd = 1)\n  \n  # implement GWAS\n  pvals <- c()\n  for(i in 1:1,651,345){\n    mod <- lm(y ~ snp[,i])\n    pvals[i] <- tidy(mod)$p.value[2]\n  }\n  # record smallest p-value\n  min(pvals)\n}\n\n# Run code with mclapply()\nset.seed(494)\nsimresmclapply <- mclapply(1:1000, do_one_sim, mc.cores = 2) \n\n#will print quantile\nquantile(simresmclapply %>% as.data.frame(), 0.05)\n\nUsing PLINK for Multiple Hypothesis Testing\nSince the multiple testing process is so computationally expensive in RStudio, I ran it in PLINK. Running 1,000 replications on this entire dataset took only about 20 minutes in PLINK versus 20 days in RStudio, which is obviously a huge reduction in computational time (1440x faster).\nTo do multiple testing in PLINK, complete the following steps:\nLoad the data into R.\n\n\nload(\"rbloggersData/conversionTable.RData\")\n\npathM <- paste(\"rbloggersData/108Malay_2527458snps\", c(\".bed\", \".bim\", \".fam\"), sep = \"\")\nSNP_M <- read.plink(pathM[1], pathM[2], pathM[3])\n\npathI <- paste(\"rbloggersData/105Indian_2527458snps\", c(\".bed\", \".bim\", \".fam\"), sep = \"\")\nSNP_I <- read.plink(pathI[1], pathI[2], pathI[3])\n\npathC <- paste(\"rbloggersData/110Chinese_2527458snps\", c(\".bed\", \".bim\", \".fam\"), sep = \"\")\nSNP_C <- read.plink(pathC[1], pathC[2], pathC[3])\n\nrbloggers_fam <- rbind(SNP_M$fam, SNP_I$fam, SNP_C$fam)\nrbloggers_map <- rbind(SNP_M$map, SNP_I$map, SNP_C$map)\n\n\nIf you didn’t run the GWAS in PLINK earlier, you will need to merge the 9 files (the .bim, .bam, and .fam for each of the 3 sub-populations). I listed the name of each of these files in .txt file with the structure shown in the image below.\n\nIn PLINK, then run the command ./plink --merge-list allfiles.txt --make-bed --out rbloggersComb. This combines the data into three files that PLINK can use (rbloggersComb.bed, rbloggersComb.bim, rbloggersComb.fam).\nCreate the function create_quantitative_trait() that creates a null trait. Then, bind the 1000 trait columns to the pedigree and member columns from the rbloggers_fam table. Write this file to the folder where the data and allfiles.txt file is stored.\n\n\ncreate_quantitative_trait <- function(i){\n  y <- rnorm(n = 323, mean = 0, sd = 1)\n}\n\ntraits <- as.data.frame(replicate(1000, create_quantitative_trait()))\n\nrbloggers_Poptraits <- cbind(rbloggers_fam %>%\n        dplyr::select(1:2), traits)\n\nwrite_delim(rbloggers_Poptraits, \"rbloggersSep/rbloggers_Poptraits\")\n\n\nIn PLINK, run the command ./plink --bfile rbloggersComb --assoc --pheno rbloggers_Poptraits --all-pheno --pfilter 1e-3\nThis will essentially run a GWAS 1000 times and it will take all p-values from those tests that are less than 1e-3. We will not end up needing all 1.65 million p-values that come with each association test (we just need the smallest one) so this lowers the computational burden on the computer.\nOnce PLINK finishes this command (somewhere between 15-25 minutes), read all the files into R using the following code. This should take only a couple minutes.\n\n\ndataFiles <- lapply(Sys.glob(\"rbloggersSep/plink.P*.qassoc\"), read_table)\n\n\nRun the code below. This will take the smallest p-value from each of the 1000 genetic-wide association studies, and then take the 5% quantile those 1000 smallest p-values. This represents the threshold you should use when running a GWAS on your real data with an actual trait of interest.\n\n\npvalues <- sapply(dataFiles, function(x) min(x$P, na.rm=TRUE))\nquantile(pvalues, 0.05)\n\n\nDue to storage space on my computer, I deleted all the 1000 files before creating this website so the threshold doesn’t print out above, but it was \\(5.424 \\times 10^{-8}\\)! If you remember from above, the Bonferroni corrected threshold was \\(3.03 \\times 10^{-8}\\). Due to tests being correlated, we are effectively conducting fewer tests which results in the simulated threshold being higher than the Bonferroni corrected threshold which treats the tests independently. The Bonferroni correction is essentially slightly too conservative - it suggests a significance threshold that is smaller than we truly need it to be. As a result, we get more type II errors and lower power. As I mentioned earlier, the harm of type II errors is that they conclude a SNP is has no relationship to the trait of interest when in reality it does, causing researchers to potentially miss out on key information in helping solve a disease. Thus, if you have time to determine a threshold using simulation instead of Bonferroni, I recommend doing so.\n\n\n\n“Linkage Disequilibrium.” n.d. Linkage Disequilibrium - an Overview | ScienceDirect Topics. https://www.sciencedirect.com/topics/neuroscience/linkage-disequilibrium.\n\n\n\n\n",
      "last_modified": "2022-12-10T17:38:43-06:00"
    },
    {
      "path": "pca.html",
      "title": "Genetic Ancestry & PCA",
      "description": "What genetic ancestry is and how to use PCA to incorporate it into statistical analyses",
      "author": [],
      "contents": "\nWhat is Genetic Ancestry?\nWhile so far I have mostly talked about using statistics to identify SNPs associated with a particular trait, this is not the only thing statistical geneticists do. Companies like 23andMe or AncestryDNA profit by selling DNA testing kits and returning the customer with a comprehensive ancestry breakdown. How do they do this?\nWhat genetic ancestry does is look at the ancestral origin of the genetic material we actually inherited. While we often think of being “1/4 Irish” or “1/8 North African”, we don’t actually inherit exactly 1/4 of our grandparents’ genetic material. As DNA is passed from one generation to the next, recombination events happen that shuffle up the genetic material. So while we have the same genealogical ancestry as our siblings, genetic ancestry isn’t the same. We can think of our genetic ancestry in two ways - locally and globally. Local ancestry refers to inheriting a specific part of your genome from an ancestor of a particular category. Global ancestry looks across the whole genome and refers to inheriting a proportion of your genome from an ancestor of a particular category.\nThere are difficulties to determining someone’s local and global ancestry. It is hard to figure out which ancestor you got each portion of your genome from, and you need good information about which ancestry categories those ancestors belonged to. Due to these challenges, the focus is more on which segments of your genome are similar to individuals from a specific category. There are still challenges with this, which include how to define “similarity”, at what level to define categories (Swedish vs Scandinavian vs European), and finding people from each category to compare your genome to. The two large classes of statistical methods that are used to infer genetic similarity are machine learning tools (including PCA, regression forests, and support vector machines) and model-based, biologically informed approaches (including bayesian methods, maximum likelihood estimation, and others).\nThe Genetic Ancestry of African Americans, Latinos, and European Americans across the U.S.\nTo learn more about genetic ancestry, we read a paper in class titled “The Genetic Ancestry of African Americans, Latinos, and European Americans Across the United States” published in 2015 and written by Katarzyna Bryc, Eric Y. Durand, Michael J. Macpherson, David Reich, and Joanna Mountain. This study looks at the genetic ancestry of 5,269 African-Americans, 8,663 Latinos, and 148,789 European Americans who are 23andMe customers and shows that historical interactions between the groups are present in the genetic-ancestry of Americans today. This paper was very dense, but I had a few main takeaways from it.\nFirst, the authors used what they called “Ancestry Composition” which assigns ancestry labels to short local phased genomic regions and produces local ancestry proportions and confidence scores. This method provides ancestry proportions for several worldwide populations at each window the genome. If a window has a majority of a continental ancestry (>51%), that window is counted in the number of windows inherited from the ancestry. So to estimate the proportion of a particular ancestry someone is, you divide the number of windows of that ancestry they have by the total number of windows studied.\nThe authors also wanted to understand the time frame of admixture events, and to do so they used simple admixture models and grid search optimization. With this method and their ancestry composition method described above, they were able to come up with some interesting results. They were able to find differences in ancestry proportions between slave and free states and learned 1/5 of Africans have Native American ancestry. For the Latino group, they saw high levels of Iberian ancestry appear in areas of high Native American ancestry. Additionally, the noted that European ancestry not homogenous across US, which likely reflects immigration patterns of different ethnic groups.\nGenetic Ancestry and its confounding role in GWAS\nGenetic ancestry is not only studied to determine whether or not we can see historical interactions present in the genetics of Americans today. Genetic ancestry is also important because it is a potential confounding variable in GWAS. When we are trying to determine if a particular SNP has a relationship with our trait of interest, we have to keep in mind the role of ancestry. Ancestry has a relationship with genotypes because the allele frequency of the SNP we’re testing differs across ancestral populations. Additionally, ancestry can have a relationship with our trait of interest - environmental factors or causal SNPs in other parts of the genome can differ across ancestry groups.\nKnowing that genetic ancestry is a confounding variable, we should adjust for it in our GWAS models with the following equation, where \\(y\\) is the trait, \\(x_j\\) is the number of minor alleles at position \\(j\\), and \\(\\pi\\) is the genetic ancestry.\n\\[E[y|x_j, \\pi] = \\alpha + \\beta_j x_j + \\gamma\\pi \\\\\\]\nBefore completing the GWAS, we will need to infer genetic ancestry using one of the methods mentioned earlier. Here we will use PCA.\nPCA background\nPrincipal component analysis (PCA) is a widely used technique for dimension reduction. Dimension reduction aims to represent the information within our data with fewer variables, which is perfect for genetic data where we have millions of SNPs. With PCA, we are specifically looking for linear transformation of our data that explains the most variability. This linear representation is composed of principal components, or PCs. These PCs are new variables that are a linear combinations of our original SNPs:\n\\[PC_1 = a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1p}x_p\\]\n\\[PC_2 = a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2p}x_p\\]\n\\[....\\]\n\\[PC_p = a_{p1}x_1 + a_{p2}x_2 + \\cdots + a_{pp}x_p\\]\nThe number of the PC has some meaning to it - \\(PC_1\\) is the component that explains the most variability in our data when all possible linear combinations are considered. In other words, it has the highest variance. \\(PC_2\\) has the next highest variance, subject to the constraint of being orthogonal to, or uncorrelated with, \\(PC_1\\). Next is \\(PC_3\\), which is orthogonal to \\(PC_2\\), and so forth.\nOther important terminology related to PCs are scores and loadings. Loadings are the coefficients \\(a_{11}, a_{22}\\), etc, which represent the contribution of each of the original variables to the new PC. Scores are the values that PCs take when you multiply the loading \\(a_{pp}\\) by the value at that SNP, \\(x_p\\).\nRunning PCA on our data\nTo learn how to run PCA on our dataset, I largely followed this tutorial by Xiuwen Zheng from the Department of Biostatistics at the University of Washington – Seattle.\nStart by loading necessary libraries. If you have trouble installing the SNPRelate package, make sure you follow how to do it exactly as shown in the tutorial.\n\n\nlibrary(snpStats)\nlibrary(tidyverse)\nlibrary(gdsfmt)\nlibrary(SNPRelate)\n\n\nThe next step is to load the data, convert it to GDS format, and combine files. Once you do this once, it will create files on your computer and you do not have to run this code again.\n\n\nbed.fn.m <- \"/Users/erinfranke/Desktop/GWAStutorial-master/public/Genomics/108Malay_2527458snps.bed\"\nfam.fn.m <- \"/Users/erinfranke/Desktop/GWAStutorial-master/public/Genomics/108Malay_2527458snps.fam\"\nbim.fn.m <- \"/Users/erinfranke/Desktop/GWAStutorial-master/public/Genomics/108Malay_2527458snps.bim\"\n\nsnpgdsBED2GDS(bed.fn.m, fam.fn.m, bim.fn.m, \"test.gds\")\n\nbed.fn.i <- \"/Users/erinfranke/Desktop/GWAStutorial-master/public/Genomics/105Indian_2527458snps.bed\"\nfam.fn.i <- \"/Users/erinfranke/Desktop/GWAStutorial-master/public/Genomics/105Indian_2527458snps.fam\"\nbim.fn.i <- \"/Users/erinfranke/Desktop/GWAStutorial-master/public/Genomics/105Indian_2527458snps.bim\"\n\nsnpgdsBED2GDS(bed.fn.i, fam.fn.i, bim.fn.i, \"test2.gds\")\n\nbed.fn.c <- \"/Users/erinfranke/Desktop/GWAStutorial-master/public/Genomics/110Chinese_2527458snps.bed\"\nfam.fn.c <- \"/Users/erinfranke/Desktop/GWAStutorial-master/public/Genomics/110Chinese_2527458snps.fam\"\nbim.fn.c <- \"/Users/erinfranke/Desktop/GWAStutorial-master/public/Genomics/110Chinese_2527458snps.bim\"\n\nsnpgdsBED2GDS(bed.fn.c, fam.fn.c, bim.fn.c, \"test3.gds\")\n\nsnpgdsSummary(\"test.gds\")\n\nfn1 <- \"test.gds\"\nfn2 <- \"test2.gds\"\nfn3 <- \"test3.gds\"\nsnpgdsCombineGeno(c(fn1, fn2, fn3), \"test4.gds\")\n\n\nNext, get a summary of the combined file and open it.\n\n\nsnpgdsSummary(\"test4.gds\")\n\nThe file name: /Users/erinfranke/Desktop/MACStats/Statistical Genetics/finalcontentsummary/test4.gds \nThe total number of samples: 323 \nThe total number of SNPs: 2176501 \nSNP genotypes are stored in SNP-major mode (Sample X SNP).\nThe number of valid samples: 323 \nThe number of biallelic unique SNPs: 1300388 \n\ngenofile <- snpgdsOpen(\"test4.gds\")\n\n\nEarlier in the multiple testing section I talked a little bit about correlation between SNPs and linkage disequilibrium. When doing PCA, its suggested to use a pruned set of SNPs which are in approximate linkage equilibrium with each other to avoid the strong influence of SNP clusters. The following code tries different LD thresholds and selects a set of SNPs.\n\n\nset.seed(1000)\n\n# Try different LD thresholds for sensitivity analysis\nsnpset <- snpgdsLDpruning(genofile, ld.threshold=0.2)\n\nSNP pruning based on LD:\nExcluding 0 SNP on non-autosomes\nExcluding 876,113 SNPs (monomorphic: TRUE, MAF: NaN, missing rate: NaN)\n    # of samples: 323\n    # of SNPs: 1,300,388\n    using 1 thread\n    sliding window: 500,000 basepairs, Inf SNPs\n    |LD| threshold: 0.2\n    method: composite\nChromosome 1: 4.54%, 8,114/178,671\nChromosome 2: 4.27%, 7,658/179,463\nChromosome 3: 4.58%, 6,933/151,523\nChromosome 4: 4.55%, 6,304/138,560\nChromosome 5: 4.54%, 6,051/133,424\nChromosome 6: 4.13%, 5,916/143,125\nChromosome 7: 4.59%, 5,473/119,147\nChromosome 8: 4.33%, 4,954/114,456\nChromosome 9: 4.76%, 4,637/97,371\nChromosome 10: 4.68%, 5,109/109,218\nChromosome 11: 4.20%, 4,718/112,258\nChromosome 12: 4.70%, 5,024/106,902\nChromosome 13: 5.03%, 3,724/74,013\nChromosome 14: 4.85%, 3,499/72,089\nChromosome 15: 4.97%, 3,445/69,297\nChromosome 16: 4.97%, 3,717/74,739\nChromosome 17: 5.13%, 3,515/68,572\nChromosome 18: 5.54%, 3,443/62,121\nChromosome 19: 4.99%, 2,713/54,393\nChromosome 20: 5.30%, 2,841/53,560\nChromosome 21: 5.32%, 1,613/30,341\nChromosome 22: 5.25%, 1,747/33,258\n101,148 markers are selected in total.\n\nstr(snpset)\n\nList of 22\n $ chr1 : chr [1:8114] \"rs11240777\" \"kgp9059038\" \"rs7537756\" \"exm628\" ...\n $ chr2 : chr [1:7658] \"kgp4971557\" \"kgp14800530\" \"rs17041717\" \"kgp2057000\" ...\n $ chr3 : chr [1:6933] \"kgp7646170\" \"kgp3611083\" \"kgp4281785\" \"kgp5157934\" ...\n $ chr4 : chr [1:6304] \"kgp7767085\" \"kgp7706308\" \"kgp10870542\" \"rs6847267\" ...\n $ chr5 : chr [1:6051] \"kgp2306997\" \"kgp9525211\" \"kgp7091813\" \"kgp10311375\" ...\n $ chr6 : chr [1:5916] \"kgp22745386\" \"kgp22734311\" \"kgp22752232\" \"kgp22744524\" ...\n $ chr7 : chr [1:5473] \"kgp3293955\" \"kgp2819807\" \"kgp4314509\" \"kgp6561807\" ...\n $ chr8 : chr [1:4954] \"kgp8876035\" \"kgp5401210\" \"kgp12018201\" \"kgp1622190\" ...\n $ chr9 : chr [1:4637] \"rs680654\" \"kgp3928777\" \"kgp3538210\" \"kgp4962798\" ...\n $ chr10: chr [1:5109] \"kgp10879287\" \"rs3125037\" \"kgp22052249\" \"kgp21937175\" ...\n $ chr11: chr [1:4718] \"exm869284\" \"rs2272566\" \"kgp6770543\" \"kgp9403358\" ...\n $ chr12: chr [1:5024] \"kgp8962939\" \"rs1106984\" \"rs7135126\" \"rs7974784\" ...\n $ chr13: chr [1:3724] \"kgp6812842\" \"kgp1456930\" \"kgp200360\" \"rs9580112\" ...\n $ chr14: chr [1:3499] \"kgp3297105\" \"exm1083420\" \"Exome_Asian_chr14-20296278-19\" \"kgp11118585\" ...\n $ chr15: chr [1:3445] \"kgp1349873\" \"rs4932072\" \"kgp6048141\" \"Exome_Asian_chr15-20588685-19\" ...\n $ chr16: chr [1:3717] \"kgp4996586\" \"kgp11473370\" \"kgp571139\" \"kgp9353104\" ...\n $ chr17: chr [1:3515] \"kgp9690030\" \"kgp942802\" \"kgp7899711\" \"rs7223759\" ...\n $ chr18: chr [1:3443] \"rs8096369\" \"kgp27746\" \"rs621636\" \"kgp5913270\" ...\n $ chr19: chr [1:2713] \"kgp21362931\" \"kgp4747527\" \"kgp574339\" \"kgp12372026\" ...\n $ chr20: chr [1:2841] \"rs6139074\" \"rs6039403\" \"kgp8039653\" \"kgp10171668\" ...\n $ chr21: chr [1:1613] \"rs12627229\" \"exm1562172\" \"kgp13170864\" \"kgp11190641\" ...\n $ chr22: chr [1:1747] \"kgp1568720\" \"rs5747010\" \"kgp8641560\" \"rs16980739\" ...\n\nnames(snpset)\n\n [1] \"chr1\"  \"chr2\"  \"chr3\"  \"chr4\"  \"chr5\"  \"chr6\"  \"chr7\"  \"chr8\" \n [9] \"chr9\"  \"chr10\" \"chr11\" \"chr12\" \"chr13\" \"chr14\" \"chr15\" \"chr16\"\n[17] \"chr17\" \"chr18\" \"chr19\" \"chr20\" \"chr21\" \"chr22\"\n\n# Get all selected snp id\nsnpset.id <- unlist(unname(snpset))\nhead(snpset.id)\n\n[1] \"rs11240777\" \"kgp9059038\" \"rs7537756\"  \"exm628\"     \"rs1891910\" \n[6] \"kgp7634233\"\n\nWe can then run PCA and calculate the percent of variation is accounted for by the top principal components. It looks like the first principal component explains 3.13% of variation, the second explains 0.85%, the third and fourth explain 0.43%, the fifth 0.39%, etc. Therefore, the optimal number of principal components to use is probably 2.\n\n\n# Run PCA\npca <- snpgdsPCA(genofile, snp.id=snpset.id, num.thread=2)\n\nPrincipal Component Analysis (PCA) on genotypes:\nExcluding 2,075,353 SNPs (non-autosomes or non-selection)\nExcluding 0 SNP (monomorphic: TRUE, MAF: NaN, missing rate: NaN)\n    # of samples: 323\n    # of SNPs: 101,148\n    using 2 threads\n    # of principal components: 32\nPCA:    the sum of all selected genotypes (0,1,2) = 10950024\nCPU capabilities: Double-Precision SSE2\nTue Dec  6 15:32:23 2022    (internal increment: 1508)\n\n[..................................................]  0%, ETC: ---        \n[==================================================] 100%, completed, 2s\nTue Dec  6 15:32:25 2022    Begin (eigenvalues and eigenvectors)\nTue Dec  6 15:32:25 2022    Done.\n\n# variance proportion (%)\npc.percent <- pca$varprop*100\nhead(round(pc.percent, 2),20)\n\n [1] 3.13 0.85 0.43 0.43 0.39 0.38 0.38 0.37 0.37 0.37 0.37 0.37 0.36\n[14] 0.36 0.36 0.36 0.36 0.36 0.36 0.36\n\nIf we didn’t have any prior population information, we could plot the first two principal components and see if we see any patterns in the data. It kind of looks like we have three separate populations, which we know to be true!\n\n\n# make a data.frame\ntab <- data.frame(sample.id = pca$sample.id,\n    EV1 = pca$eigenvect[,1],    # the first eigenvector\n    EV2 = pca$eigenvect[,2],    # the second eigenvector\n    stringsAsFactors = FALSE)\n\n# Draw\nplot(tab$EV2, tab$EV1, xlab=\"eigenvector 2\", ylab=\"eigenvector 1\")\n\n\n\nTo incorporate prior population, first load the data:\n\n\nload(\"/Users/erinfranke/Desktop/GWAStutorial-master/conversionTable.RData\")\n\npathM <- paste(\"/Users/erinfranke/Desktop/GWAStutorial-master/public/Genomics/108Malay_2527458snps\", c(\".bed\", \".bim\", \".fam\"), sep = \"\")\nSNP_M <- read.plink(pathM[1], pathM[2], pathM[3])\n\npathI <- paste(\"/Users/erinfranke/Desktop/GWAStutorial-master/public/Genomics/105Indian_2527458snps\", c(\".bed\", \".bim\", \".fam\"), sep = \"\")\nSNP_I <- read.plink(pathI[1], pathI[2], pathI[3])\n\npathC <- paste(\"/Users/erinfranke/Desktop/GWAStutorial-master/public/Genomics/110Chinese_2527458snps\", c(\".bed\", \".bim\", \".fam\"), sep = \"\")\nSNP_C <- read.plink(pathC[1], pathC[2], pathC[3])\n\n\nThen, run the following code. We can see the three clusters align with the three subpopulations as we expected.\n\n\nSNP_fam <- rbind(SNP_M$fam, SNP_I$fam, SNP_C$fam) %>%\n  mutate(pop = c(rep(\"Malay\", 108), rep(\"Indian\", 105), rep(\"Chinese\", 110)))\n\nsample.id =  SNP_fam$pedigree\n\n\ntab <- data.frame(sample.id = SNP_fam$pedigree,\n    pop = factor(SNP_fam$pop)[match(pca$sample.id, sample.id)],\n    EV1 = pca$eigenvect[,1],    # the first eigenvector\n    EV2 = pca$eigenvect[,2],    # the second eigenvector\n    stringsAsFactors = FALSE)\nhead(tab)\n\n  sample.id   pop         EV1        EV2\n1 M11062205 Malay -0.03507693 0.08154056\n2 M11050406 Malay -0.03538905 0.09150315\n3 M11050407 Malay -0.03566775 0.08688487\n4 M11070602 Malay -0.03317120 0.08035539\n5 M11060912 Malay -0.01784295 0.05161642\n6 M11060920 Malay -0.02643969 0.05688225\n\nplot(tab$EV2, tab$EV1, col=as.integer(tab$pop), xlab=\"eigenvector 2\", ylab=\"eigenvector 1\")\nlegend(\"bottomright\", legend=levels(tab$pop), pch=\"o\", col=1:nlevels(tab$pop))\n\n\n\nIf we wanted to, we could look at the top 4 PCs with the following code and decide whether or not it is worth including PCs 3 & 4 when accounting for genetic ancestry in GWAS. To me, it looks like the clusters overlap a lot less (meaning more variation is explained) when looking at PCs 1 & 2 and PCs 3 & 4 don’t tell us that much (for example, all the dots are on top of each other for the plot of PC3 versus PC4).\n\n\nlbls <- paste(\"PC\", 1:4, \"\\n\", format(pc.percent[1:4], digits=2), \"%\", sep=\"\")\npairs(pca$eigenvect[,1:4], col=tab$pop, labels=lbls)\n\n\n\nWe can also see this in a parallel coordinates plot for the top 16 principal components - starting at PC3 the green sub population (Malay) is completely covered by the red and black lines (Chinese and Indian). This indicates any PCs beyond 1 & 2 are really not helpful significantly to explain variation.\n\n\nlibrary(MASS)\n\ndatapop <- factor(SNP_fam$pop)[match(pca$sample.id, sample.id)]\nparcoord(pca$eigenvect[,1:16], col = datapop)\n\n\n\nIncorporating PCA into GWAS\nHaving completed PCA, we can now incorporate PC1 and PC2 into our GWAS. This allows us to adjust for the confounding role that ancestry plays in identifying relationships between SNPs and the trait of interest.\nThis is important because if I were to create a trait that is correlated with both population and a particular SNP, we would expect the p-value for that particular SNP to be significant and easily identifiable. However, when we don’t include the top PCs in our marginal regression models we may get additional SNPs that have significant p-values, specifically SNPs that differ the most in minor allele frequency between the three groups. Including PC1 and PC2 will better account for these ancestral differences and reduce the probability of spurious associations (2+ variables are associated but not causally related due to an unseen factor).\nThe first step to including principal components in our GWAS is to again convert the SnpMatrix to numeric and generate a trait based on the causal SNP.\n\n\nrbloggers_fam <- rbind(SNP_M$fam, SNP_I$fam, SNP_C$fam)\nSNP <- rbind(SNP_M$genotypes, SNP_I$genotypes, SNP_C$genotypes)\n\nX <- as(SNP, \"numeric\")\n\nset.seed(494)\ny <- X[,'rs3131972'] + rnorm(323, 0, 1.5)\n\n\nI then took the member and pedigree columns on the 323 individuals and created a trait varying based on sub population. After that, I added the trait based on the causal SNP rs3131972 to the trait varying based on sub population to create a trait that varies based on sub population AND one SNP. I sent this file to where the data is stored.\n\n\nset.seed(494)\nrbloggers_popSNP <- cbind(rbloggers_fam %>% dplyr::select(1:2), trait = c(rnorm(n = 108, mean = 0, sd = 1), rnorm(105, -1, sd = 1), rnorm(110, 1, 1)), y) %>%\n  mutate(trait = trait + y)\n\nwrite_delim(rbloggers_popSNP, \"/Users/erinfranke/Desktop/MACStats/Statistical Genetics/StatGenWillErin/rbloggersSep/rbloggers_popSNP\")\n\n\nTo learn how to incorporate principal components in PLINK, I found this set of exercises from Tufts University.\nI started by running a GWAS in PLINK without PCs using the code ./plink --bfile rbloggersComb --assoc --pheno rbloggers_popSNP --maf 0.05 --out gwasNoPCA. Next, I read the results into RStudio and created a QQ plot and got a lambda value. This lambda represents the genomic control (GC), which measures the extent of inflation of association based test statistics due to population stratification or other confounders. A value of \\(\\lambda_{GC} \\approx 1\\) indicates no stratification, while typically \\(\\lambda_{GC} > 1.05\\) indicates stratification or other confounders including family structure, cryptic relatedness, or differential bias (spurious differences in allele frequencies between cases and controls due to differences in sample collection, sample preparation, and/or genotyping assay procedures). With a \\(\\lambda_{GC}\\) of 1.73 and the QQ plot not fitting the expected distribution, its clear that there is stratification in this data and our p-values are inflated. Hopefully PCA can help account for this.\n\n\nresults_noPCA <- read_table(\"/Users/erinfranke/Desktop/MACStats/Statistical Genetics/StatGenWillErin/rbloggersSep/gwasNoPCA.qassoc\") %>%\n  arrange(P)\n\nhead(results_noPCA)\n\n# A tibble: 6 × 10\n    CHR SNP            BP NMISS  BETA    SE    R2     T        P X10  \n  <dbl> <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl> <lgl>\n1     2 rs13403822 1.73e8   323  1.32 0.200 0.119  6.58 1.97e-10 NA   \n2     2 kgp779520  1.73e8   323  1.28 0.202 0.112  6.36 7.14e-10 NA   \n3     2 kgp5773487 1.10e8   322  1.56 0.247 0.111  6.32 8.95e-10 NA   \n4     2 rs12624292 1.73e8   323  1.26 0.202 0.108  6.23 1.49e- 9 NA   \n5     2 rs11892858 1.73e8   323  1.22 0.200 0.104  6.12 2.75e- 9 NA   \n6    13 kgp4920926 2.62e7   323 -1.34 0.223 0.101 -6.01 5.15e- 9 NA   \n\nqq.chisq(-2 * log(results_noPCA$P), df = 2, pvals = TRUE, overdisp = FALSE, thin = c(0.8, 1000))\n\n\n           N      omitted       lambda \n1.200898e+06 0.000000e+00 1.731327e+00 \n\nTo run a GWAS with PCs, I created a pcs.txt file that includes member, pedigree, pc1, and pc2 with the following code and stored it with the rest of my data. To learn how to structure that file correctly, I looked at these slides from Colorado University.\n\n\npcs <- cbind(rbloggers_fam %>% dplyr::select(1:2), pc1 = tab$EV1, pc2 = tab$EV2)\nhead(pcs)\n\n           pedigree    member         pc1        pc2\nM11062205 M11062205 M11062205 -0.03507693 0.08154056\nM11050406 M11050406 M11050406 -0.03538905 0.09150315\nM11050407 M11050407 M11050407 -0.03566775 0.08688487\nM11070602 M11070602 M11070602 -0.03317120 0.08035539\nM11060912 M11060912 M11060912 -0.01784295 0.05161642\nM11060920 M11060920 M11060920 -0.02643969 0.05688225\n\nwrite_delim(pcs, \"/Users/erinfranke/Desktop/MACStats/Statistical Genetics/StatGenWillErin/rbloggersSep/pcs\")\n\n\nThen, I ran ./plink --bfile rbloggersComb --assoc --covar pcs.txt --covar-name pc1, pc2 --pheno rbloggers_popSNP --out gwasPCA in PLINK and read the results into RStudio below. We can see that the \\(\\lambda_{GC}\\) value went down from 1.73 to 1.61. This means that genetic ancestry did have some kind of confounding effect on the relationship between the SNPs and our trait of interest, so it is good we included them.\n\n\nresults_PCA <- read_table(\"/Users/erinfranke/Desktop/MACStats/Statistical Genetics/StatGenWillErin/rbloggersSep/gwasPCA.qassoc\") %>%\n  arrange(P)\nhead(results_PCA)\n\n# A tibble: 6 × 10\n    CHR SNP            BP NMISS  BETA    SE    R2     T        P X10  \n  <dbl> <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl> <lgl>\n1     2 rs13403822 1.73e8   323  1.32 0.200 0.119  6.58 1.97e-10 NA   \n2     2 kgp779520  1.73e8   323  1.28 0.202 0.112  6.36 7.14e-10 NA   \n3     2 kgp5773487 1.10e8   322  1.56 0.247 0.111  6.32 8.95e-10 NA   \n4     2 rs12624292 1.73e8   323  1.26 0.202 0.108  6.23 1.49e- 9 NA   \n5     2 rs11892858 1.73e8   323  1.22 0.200 0.104  6.12 2.75e- 9 NA   \n6    13 kgp4920926 2.62e7   323 -1.34 0.223 0.101 -6.01 5.15e- 9 NA   \n\nqq.chisq(-2 * log(results_PCA$P), df = 2, pvals = TRUE, overdisp = FALSE, thin = c(0.8, 1000))\n\n\n           N      omitted       lambda \n1.651345e+06 0.000000e+00 1.613123e+00 \n\nOne method to correct for inflated \\(\\lambda_{GC}\\) values is to divide association statistics by \\(\\lambda_{GC}\\). This usually provides a sufficient correction for stratification in the case of genetic drift, meaning random fluctuations in allele frequencies over time due to sampling effects, particularly in small populations. However, in the case of ancient population divergence (when populations accumulate independent genetic mutations overtime time and become more different), dividing by \\(\\lambda_{GC}\\) is unlikely to be adequate because SNPs with unusual allele frequency differences that lie outside the expected distribution could be this way because of natural selection. Therefore, there need to be additional approaches to accounting for population stratification than dividing association statistics by \\(\\lambda_{GC}\\). These approaches include PCA, but also family based association tests and perhaps most effectively mixed models with PC covariates. I won’t talk about these now but it is something I’m interested in looking more into in the future!\n\n\n\n",
      "last_modified": "2022-12-06T15:34:19-06:00"
    },
    {
      "path": "readings.html",
      "title": "Journal Articles & Ethics",
      "description": "stuff about readings",
      "author": [],
      "contents": "\nR Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\nIncluding Plots\nYou can also embed plots, for example:\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.\n\n\n\n",
      "last_modified": "2022-12-01T16:12:33-06:00"
    }
  ],
  "collections": []
}
